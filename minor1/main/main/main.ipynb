{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    "\n",
    "import std::string\n",
    "\n",
    "vocab = list(\"@abcdefghijklmnopqrstuvwxyz'?!123456789  \")\n",
    "\n",
    "char_to_num = {char: num for num, char in enumerate(vocab)}\n",
    "num_to_char = {num: char for num, char in enumerate(vocab)}\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 128, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(128, 256, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.conv3 = nn.Conv3d(256, 75, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        self.bidirectional_lstm1 = nn.LSTM(6375, 128, bidirectional=True, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.bidirectional_lstm2 = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dense = nn.Linear(256, 41)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = self.flatten(x)\n",
    "        x, _ = self.bidirectional_lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.bidirectional_lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.softmax(self.dense(x), dim = 1)\n",
    "        return x\n",
    "\n",
    "def ctc_loss(preds, targets, input_lengths, target_lengths):\n",
    "    # CTC loss calculation\n",
    "    loss = nn.CTCLoss(blank=0)(preds, targets, input_lengths, target_lengths)\n",
    "    return loss\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets, input_lengths, target_lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        # Assuming targets is a tensor containing the target sequences\n",
    "        # You might need to process your targets to fit the CTC loss requirements\n",
    "        loss = ctc_loss(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Initialize your model\n",
    "model = MyModel()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "# Initialize optimizer and other hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 1\n",
    "\n",
    "def load_alignments(path:str)\n",
    "    with open(path, 'r') as f: \n",
    "        lines = f.readlines() \n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if line[2] != 'sil': \n",
    "            tokens = [*tokens,line[2]]\n",
    "        stra = ' '.join(tokens)\n",
    "    return torch.tensor(char_to_num[s] for s in stra)\n",
    "\n",
    "\n",
    "# Assuming you have a DataLoader named train_dataloader\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss}')\n",
    "\n",
    "# Remember to adjust hyperparameters and data loading according to your specific dataset and requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4830000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error: Data size does not match expected array size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mw:\\Projects\\Minor Project 1\\minor1\\main\\main\\main.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Projects/Minor%20Project%201/minor1/main/main/main.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(total_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Projects/Minor%20Project%201/minor1/main/main/main.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m total_size:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/Projects/Minor%20Project%201/minor1/main/main/main.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mError: Data size does not match expected array size.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Projects/Minor%20Project%201/minor1/main/main/main.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m tensor5D \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mview(dimensions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/Projects/Minor%20Project%201/minor1/main/main/main.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Now tensor5D is a 5D tensor with the data from the binary file\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Error: Data size does not match expected array size."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define dimensions\n",
    "dim1 = 10\n",
    "dim2 = 75\n",
    "dim3 = 46\n",
    "dim4 = 140\n",
    "dim5 = 1\n",
    "\n",
    "# Read binary data into a NumPy array\n",
    "with open(\"data10.bin\", \"rb\") as file:\n",
    "    data = torch.from_numpy(np.frombuffer(file.read(), dtype=np.float64))\n",
    "\n",
    "# Reshape the data into a 5D tensor\n",
    "dimensions = (dim1, dim2, dim3, dim4, dim5)\n",
    "total_size = np.prod(dimensions)\n",
    "print(total_size)\n",
    "if data.numel() != total_size:\n",
    "    raise ValueError(\"Error: Data size does not match expected array size.\")\n",
    "\n",
    "tensor5D = data.view(dimensions)\n",
    "\n",
    "# Now tensor5D is a 5D tensor with the data from the binary file\n",
    "print(tensor5D[0][0][0][0][0])  # Accessing a specific element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    vector<std::string> readLinesFromFile(const std::string& filePath) {\n",
    "        ifstream file(filePath);\n",
    "        vector<std::string> lines;\n",
    "        if (file.is_open()) {\n",
    "            std::string line;\n",
    "            while (getline(file, line)) {\n",
    "                lines.push_back(line);\n",
    "            }\n",
    "            file.close();\n",
    "        }\n",
    "        else {\n",
    "            cerr << \"Unable to open file\" << endl;\n",
    "        }\n",
    "        return lines;\n",
    "    }\n",
    "\n",
    "    torch::Tensor load_video(const std::string& path) {\n",
    "        cv::VideoCapture cap(path);\n",
    "        if (!cap.isOpened()) {\n",
    "            cerr << \"Error opening video file\" << endl;\n",
    "            exit(-1);\n",
    "        }\n",
    "\n",
    "        vector<cv::Mat> frames;\n",
    "        cv::Mat frame;\n",
    "        while (cap.read(frame)) {\n",
    "            cv::cvtColor(frame, frame, cv::COLOR_BGR2GRAY);\n",
    "            frame = frame(cv::Rect(80, 190, 140, 46)); // Cropping the frame\n",
    "            frames.push_back(frame);\n",
    "        }\n",
    "        cap.release();\n",
    "\n",
    "        // Convert frames to type CV_32F for processing\n",
    "        vector<cv::Mat> processed_frames;\n",
    "        for (const auto& f : frames) {\n",
    "            cv::Mat temp;\n",
    "            f.convertTo(temp, CV_32F);\n",
    "            temp = (temp - temp.mean()) / temp.std();\n",
    "            processed_frames.push_back(temp);\n",
    "        }\n",
    "\n",
    "        // Create a tensor from the processed frames\n",
    "        auto options = torch::TensorOptions().dtype(torch::kFloat32).device(torch::kCPU).layout(torch::kStrided).requires_grad(false);\n",
    "        torch::Tensor tensor = torch::from_blob(processed_frames.data(), { static_cast<long>(frames.size()), 46, 140 }, options).clone();\n",
    "\n",
    "        return tensor;\n",
    "    }\n",
    "\n",
    "    torch::Tensor char_to_num(const vector<char>& chars, const vector<char>& vocab) {\n",
    "        map<char, int> char_to_numv;\n",
    "        for (size_t i = 0; i < vocab.size(); ++i) {\n",
    "            char_to_numv[vocab[i]] = i;\n",
    "        }\n",
    "        vector<int> indices;\n",
    "        for (char c : chars) {\n",
    "            indices.push_back(char_to_numv[c]);\n",
    "        }\n",
    "        return torch::tensor(indices, torch::kLong);\n",
    "    }\n",
    "\n",
    "    vector<char> num_to_char(const torch::Tensor& nums, const vector<char>& vocab) {\n",
    "        map<int, char> num_to_charv;\n",
    "        for (size_t i = 0; i < vocab.size(); ++i) {\n",
    "            num_to_charv[i] = vocab[i];\n",
    "        }\n",
    "        vector<char> chars;\n",
    "        for (auto num : nums) {\n",
    "            chars.push_back(num_to_charv[num.item<int>()]);\n",
    "        }\n",
    "        return chars;\n",
    "    }\n",
    "\n",
    "    vector<std::string> split(std::string line, char del) {\n",
    "        vector<std::string> splits;\n",
    "        std::string str = \"\";\n",
    "        for (char c : line) {\n",
    "            str += c;\n",
    "            if (c == del) {\n",
    "                splits.push_back(str);\n",
    "                str = \"\";\n",
    "            }\n",
    "        }\n",
    "        splits.push_back(str);\n",
    "        return splits;\n",
    "    }\n",
    "\n",
    "    vector<std::string> load_alignments(std::string path) {\n",
    "        ifstream align(path);\n",
    "        std::string line;\n",
    "        vector<std::string> lines;\n",
    "        vector<std::string> temp;\n",
    "        while (getline(align, line)) {\n",
    "            temp = split(line, ' ');\n",
    "            if (temp[2] != \"sil\") {\n",
    "                lines.push_back(temp[2]);\n",
    "            }\n",
    "        }\n",
    "        vector<char> tokens;\n",
    "        for (std::string s : lines) {\n",
    "            for (char c : s) {\n",
    "                tokens.push_back(c);\n",
    "            }\n",
    "            tokens.push_back(' ');\n",
    "        }\n",
    "        tokens.pop_back();\n",
    "        return char_to_num(tokens, vocab);\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
